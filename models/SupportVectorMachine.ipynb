{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SupportVectorMachine.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ACM-Research/fraudulent-website-detection/blob/main/SupportVectorMachine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6LR8AU6ZLp1"
      },
      "source": [
        "# Import our data here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import requests\n",
        "import re\n",
        "from google.colab import files\n",
        "\n",
        "# Load libraries\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
        "\n",
        "#sampleData = pd.read_csv('sample.csv')\n",
        "# Read the data in\n",
        "#df = pd.read_csv(\"malicious_phish.csv\")\n",
        "df = pd.read_csv('sample2.csv')\n",
        "url_list = df.url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCHs-NqwZa4_"
      },
      "source": [
        "# Rules for us to implement, or \"flags\"\n",
        "# 1.  If url contains \"%00\" \n",
        "def containsPercent00(str):\n",
        "  if str.find(\"%00\"):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# 2.  If url contains \"%01\"\n",
        "def containsPercent01(str):\n",
        "  if str.find(\"%01\"):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# 3.  If url contains '-' if more than four, then fradulent\n",
        "def containsFourDash(str):\n",
        "  if str.count(\"-\") == 4:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# 4.  If (perform get request) url returned != url sent \n",
        "def checkIf_URLsent_equals_getURL(str):\n",
        "  temp = (requests.head(\"http://\"+str).headers['location'])\n",
        "  if temp != str:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# 5.  If url contains more than one instance of \"http://\"\n",
        "def containsMoreHttp(str):\n",
        "  if str.count(\"http://\") > 1:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# 6.  If url contains more than one instance of \"https://\"\n",
        "def containsMoreHttps(str):\n",
        "  if str.count(\"https://\") > 1:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# 7.  If url contains \"http://\" AND \"https://\"\n",
        "def containsHttpAndHttps(str):\n",
        "  if \"http://\" in str and \"https://\" in str:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "# 8.  If url is over 54 characters\n",
        "def over54Chars(str):\n",
        "  if len(str) > 54:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# 9.  If url has more than one instance of a domain extension\n",
        "def hasMoreExtension(str):\n",
        "  if \".com\" in str and \".net\" in str and \".org\" in str:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "# 10. If url does not contain \"https://\"\n",
        "def doesNotContainHttps(str):\n",
        "  if \"https://\" not in str:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# 11. If url contains \"index.php\"\n",
        "def overIndexPhp(str):\n",
        "  if str.find(\"index.php\"):\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "# 12. If url has \"@\"\n",
        "def hasAt(str):\n",
        "  if \"@\" in str:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# 13. If url has \"//\" after seventh position\n",
        "def hasTwoSlash(str):\n",
        "  if str.find(\"//\")!=-1 and str.find(\"//\")>7:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "  \n",
        "# 14. If url has multiple subdomains  \n",
        "def hasMultDom(str):\n",
        "  dom = re.search(\"(?<=:\\/\\/)[^/]*\", str)\n",
        "  if(not dom): return 0\n",
        "  if(dom.group(0).count(\".\")>2): return 1\n",
        "  return 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnjG01C5ZcTs"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "\n",
        "#reading from sample file with file upload(in sidebar)\n",
        "#df = pd.read_csv('sample2.csv')\n",
        "#making pipeline to extract and adjust features for training\n",
        "url_list = df.url\n",
        "pipe = Pipeline(\n",
        "    [\n",
        "     ('count',CountVectorizer()),\n",
        "     ('tfidf',TfidfTransformer()),\n",
        "     ('scale',StandardScaler(with_mean=False)),\n",
        "     ('feature selection', VarianceThreshold())\n",
        "     ])\n",
        "pipe.fit(url_list)\n",
        "Xa = pipe.transform(url_list)\n",
        "#using above flags for 11 other features\n",
        "Xb = []\n",
        "for i in range(0,url_list.size):\n",
        "    new = []\n",
        "    new.append(containsPercent00(url_list[i]))\n",
        "    new.append(containsPercent01(url_list[i]))\n",
        "    new.append(containsFourDash(url_list[i]))\n",
        "    #new.append(checkIf_URLsent_equals_getURL(url_list[i]))\n",
        "    new.append(containsMoreHttp(url_list[i]))\n",
        "    new.append(containsMoreHttps(url_list[i]))\n",
        "    new.append(over54Chars(url_list[i]))\n",
        "    new.append(hasMoreExtension(url_list[i]))\n",
        "    new.append(doesNotContainHttps(url_list[i]))\n",
        "    new.append(overIndexPhp(url_list[i]))\n",
        "    new.append(hasAt(url_list[i]))\n",
        "    new.append(hasTwoSlash(url_list[i]))\n",
        "    new.append(hasMultDom(url_list[i]))\n",
        "    Xb.append(new)\n",
        "#cocatenating the two feature matrices\n",
        "X = hstack([Xa,csr_matrix(Xb)])\n",
        "#generating labels from data\n",
        "result = df.type\n",
        "y = []\n",
        "for i in range (0,result.size):\n",
        "    if result[i]==\"benign\": y.append(0)\n",
        "    else: y.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AixWAHwwZg7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdcecaea-9c17-4fd7-d602-0dc732c98ea5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#splitting into test and train sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0,test_size=.3)\n",
        "\n",
        "#using sci kit learn's logistic regression function to create a model\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"accuracy of support vector machine:\")\n",
        "print(clf.score(X_test,y_test))\n",
        "print(\"precision:\")\n",
        "print(precision_score(y_test,clf.predict(X_test)))\n",
        "print(\"AUC:\")\n",
        "print(roc_auc_score(y_test,clf.predict(X_test)))\n",
        "print(\"Recall:\")\n",
        "print(recall_score(y_test,clf.predict(X_test)))\n",
        "print(\"F1:\")\n",
        "print(f1_score(y_test,clf.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of support vector machine:\n",
            "0.9326666666666666\n",
            "precision:\n",
            "0.9925373134328358\n",
            "AUC:\n",
            "0.8635025043751132\n",
            "Recall:\n",
            "0.7287671232876712\n",
            "F1:\n",
            "0.8404423380726699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hACmr2_gZlQO",
        "outputId": "788de15a-afaa-4120-9f2a-797b7c442645"
      },
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1133    2]\n",
            " [  99  266]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96      1135\n",
            "           1       0.99      0.73      0.84       365\n",
            "\n",
            "    accuracy                           0.93      1500\n",
            "   macro avg       0.96      0.86      0.90      1500\n",
            "weighted avg       0.94      0.93      0.93      1500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii5oFN1Gf2aQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "de431b0b-71b4-4dc8-a732-9c2946bb3fdd"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "h = .02\n",
        "\n",
        "# we create an instance of SVM and fit out data. We do not scale our\n",
        "# data since we want to plot the support vectors\n",
        "C = 1.0  # SVM regularization parameter\n",
        "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
        "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
        "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
        "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# title for the plots\n",
        "titles = ['SVC with linear kernel',\n",
        "          'LinearSVC (linear kernel)',\n",
        "          'SVC with RBF kernel',\n",
        "          'SVC with polynomial (degree 3) kernel']\n",
        "\n",
        "\n",
        "for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
        "    # Plot the decision boundary. For that, we will assign a color to each\n",
        "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "    # Plot also the training points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "    plt.xlabel('Sepal length')\n",
        "    plt.ylabel('Sepal width')\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(titles[i])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ace9d3681fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# create a mesh to plot in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
            "\u001b[0;31mTypeError\u001b[0m: 'coo_matrix' object is not subscriptable"
          ]
        }
      ]
    }
  ]
}
